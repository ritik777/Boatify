{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests;\n",
    "from bs4 import BeautifulSoup, SoupStrainer;\n",
    "import bs4;\n",
    "#Script for scraping starts here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with page 431 Moving to the page 432\n",
      "Finished with page 432 Moving to the page 433\n",
      "Finished with page 433 Moving to the page 434\n",
      "Finished with page 434 Moving to the page 435\n",
      "Finished with page 435 Moving to the page 436\n",
      "Finished with page 436 Moving to the page 437\n",
      "Finished with page 437 Moving to the page 438\n",
      "Finished with page 438 Moving to the page 439\n",
      "Finished with page 439 Moving to the page 440\n",
      "Finished with page 440 Moving to the page 441\n",
      "Finished with page 441 Moving to the page 442\n",
      "Finished with page 442 Moving to the page 443\n",
      "Finished with page 443 Moving to the page 444\n",
      "Finished with page 444 Moving to the page 445\n",
      "Finished with page 445 Moving to the page 446\n",
      "Finished with page 446 Moving to the page 447\n",
      "Finished with page 447 Moving to the page 448\n",
      "Finished with page 448 Moving to the page 449\n",
      "Finished with page 449 Moving to the page 450\n",
      "Finished with page 450 Moving to the page 451\n",
      "Finished with page 451 Moving to the page 452\n",
      "Finished with page 452 Moving to the page 453\n",
      "Finished with page 453 Moving to the page 454\n",
      "Finished with page 454 Moving to the page 455\n",
      "Finished with page 455 Moving to the page 456\n",
      "Finished with page 456 Moving to the page 457\n",
      "Finished with page 457 Moving to the page 458\n",
      "Finished with page 458 Moving to the page 459\n",
      "Finished with page 459 Moving to the page 460\n"
     ]
    }
   ],
   "source": [
    "names_txt = []\n",
    "prices_txt = []\n",
    "location_txt = []\n",
    "zips_txt = []\n",
    "class_txt = []\n",
    "material_txt = []\n",
    "engine_txt = []\n",
    "category_txt = []\n",
    "year_txt = []\n",
    "length_txt = []\n",
    "state_txt = []\n",
    "seller_txt = []\n",
    "make_txt = []   \n",
    "    #Iterated through the set of 30 till 460 \n",
    "for j in range(431,460):\n",
    "    page = j\n",
    "    link = 'https://www.boattrader.com/boats/page-%s'%page+'/'\n",
    "    r = requests.get(link)\n",
    "    if r.status_code == 200:\n",
    "        raw_html = r.text\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    ads = soup.find_all('li', {'data-reporting-impression-listing-type': {'standard listing'}})\n",
    "    ad_links = []\n",
    "    for i in range(len(ads)):\n",
    "        ad_links.append(ads[i].a)\n",
    "    for i in range(len(ad_links)):\n",
    "        ad_link = ad_links[i]['href']\n",
    "        ad_r = requests.get(ad_link)\n",
    "        ad_html = ad_r.text\n",
    "        ad_soup = BeautifulSoup(ad_html, 'html.parser')\n",
    "        zips = ad_soup.find_all('span',{'class': 'postal-code'})\n",
    "        details = ad_soup.find_all('tr')\n",
    "        prices = ad_soup.find_all('span',{'class': 'bd-price contact-toggle'})\n",
    "        location = ad_soup.find_all('span', {'class': 'locality'})\n",
    "        state = ad_soup.find_all('abbr', {'class': 'region'})\n",
    "        seller = ad_soup.find_all('span', {'id': 'seller-name'})\n",
    "        names = ad_soup.find_all('h1',{'class': 'bd-name'})\n",
    "        if len(location)>0:\n",
    "            location_txt.append(location[0].text.encode('utf-8'))\n",
    "        else:\n",
    "            location_txt.append(b\"NA\")\n",
    "        if len(state)>0:\n",
    "            state_txt.append(state[0].text.encode('utf-8'))\n",
    "        else:\n",
    "            state_txt.append(b\"NA\")\n",
    "        if len(prices)>0:\n",
    "            prices_txt.append(prices[0].text.encode('utf-8')[13:-8])\n",
    "        else:\n",
    "            prices_txt.append(b\"NA\")\n",
    "        if len(names)>0:\n",
    "            names_txt.append(names[0].text.encode('utf-8'))\n",
    "        else:\n",
    "            names_txt.append(\"NA\")\n",
    "        if len(zips) > 0:\n",
    "            zips_txt.append(zips[0].text.encode('utf-8'))\n",
    "        else:\n",
    "            zips_txt.append(b'0')\n",
    "        if len(seller) > 0:\n",
    "            seller_txt.append(seller[0].text.encode('utf-8'))\n",
    "        else:\n",
    "            seller_txt.append(b'NA')\n",
    "        m=0\n",
    "        for k in range(len(details)):\n",
    "            if details[k].text.strip(\"\\n\")[0:5] == \"Class\":\n",
    "                class_txt.append(details[k].text.strip(\"\\n\")[6:])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:8] == \"Category\":\n",
    "                category_txt.append(details[k].text.strip(\"\\n\")[9:])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:4] == \"Year\":\n",
    "                year_txt.append(details[k].text.strip(\"\\n\")[5:])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:6] == \"Length\":\n",
    "                length_txt.append(details[k].text.strip(\"\\n\")[7:][:-1])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:13] == \"Hull Material\":\n",
    "                material_txt.append(details[k].text.strip(\"\\n\")[14:])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:11] == \"Engine Type\":\n",
    "                engine_txt.append(details[k].text.strip(\"\\n\")[12:])\n",
    "                m=m+1\n",
    "            elif details[k].text.strip(\"\\n\")[0:4] == \"Make\":\n",
    "                make_txt.append(details[k].text.strip(\"\\n\")[5:])\n",
    "                m=m+1\n",
    "        if m == 0:\n",
    "            class_txt.append(\"NA\")\n",
    "            category_txt.append(\"NA\")\n",
    "            year_txt.append(\"NA\")\n",
    "            length_txt.append(\"NA\")\n",
    "            material_txt.append(\"NA\")\n",
    "            engine_txt.append(\"NA\")\n",
    "            make_txt.append(\"NA\")\n",
    "    print(\"Finished with page\",+j,\"Moving to the page\",+(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fieldnames = ['Seller', 'Make', 'Price', 'ZIP', 'City', 'State', 'Class', 'Year', 'Length', 'Category', 'Hull Material', 'Engine Type']\n",
    "test_file = open('part14_final.csv','w', newline = '')\n",
    "csvwriter = csv.DictWriter(test_file, delimiter=',', fieldnames=fieldnames)\n",
    "csvwriter.writeheader()\n",
    "for i in range(len(material_txt)):\n",
    "    csvwriter.writerow({'Seller':seller_txt[i].decode('utf-8'),\n",
    "    'Make':make_txt[i],\n",
    "    'Price':prices_txt[i].decode('utf-8'),\n",
    "    'ZIP':zips_txt[i].decode('utf-8'),\n",
    "    'City':location_txt[i].decode('utf-8'),\n",
    "    'State':state_txt[i].decode('utf-8'),\n",
    "    'Class':class_txt[i],\n",
    "    'Year':year_txt[i],\n",
    "    'Length':length_txt[i],\n",
    "    'Category':category_txt[i],\n",
    "    'Hull Material':material_txt[i],\n",
    "    'Engine Type':engine_txt[i]})\n",
    "test_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
